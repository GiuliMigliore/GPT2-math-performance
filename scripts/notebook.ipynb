{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFLcmAxK7aYW"
      },
      "source": [
        "# LLM Evaluation\n",
        "\n",
        "In this notebook, we used the [language model evaluation harness](https://github.com/EleutherAI/lm-evaluation-harness)\n",
        "utility built by EleutherAI to evaluate our model on the GSM8k benchmark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Re0eppYizJ1"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/EleutherAI/lm-evaluation-harness\n",
        "!pip install -e lm-evaluation-harness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5T-euth7qGBA"
      },
      "source": [
        "We ran the model 20 times, changing the beam size and temperature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f6ZXRpENEPd"
      },
      "source": [
        "## Temperature = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7-G94Bb_rid"
      },
      "source": [
        "temp, beam = (0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTSKBJlVjaB-",
        "outputId": "fa155d53-fdec-446f-c33a-058d624c5892"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-04-03 20:20:32.445033: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-03 20:20:32.445097: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-03 20:20:32.447072: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-03 20:20:34.671551: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading builder script: 100% 5.67k/5.67k [00:00<00:00, 20.0MB/s]\n",
            "2024-04-03:20:20:43,662 INFO     [__main__.py:251] Verbosity set to INFO\n",
            "2024-04-03:20:20:50,913 INFO     [__main__.py:335] Selected Tasks: ['gsm8k']\n",
            "2024-04-03:20:20:50,920 INFO     [evaluator.py:131] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42\n",
            "2024-04-03:20:20:50,920 INFO     [evaluator.py:177] Initializing huggingface model, with arguments: {'pretrained': 'openai-community/gpt2', 'num_beams': 1, 'temperature': 0}\n",
            "2024-04-03:20:20:50,987 INFO     [huggingface.py:163] Using device 'cuda'\n",
            "config.json: 100% 665/665 [00:00<00:00, 3.68MB/s]\n",
            "model.safetensors: 100% 548M/548M [00:01<00:00, 339MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "generation_config.json: 100% 124/124 [00:00<00:00, 672kB/s]\n",
            "tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 141kB/s]\n",
            "vocab.json: 100% 1.04M/1.04M [00:00<00:00, 2.11MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 107MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 5.41MB/s]\n",
            "Downloading readme: 100% 7.94k/7.94k [00:00<00:00, 22.3MB/s]\n",
            "Downloading data: 100% 2.31M/2.31M [00:00<00:00, 4.85MB/s]\n",
            "Downloading data: 100% 419k/419k [00:00<00:00, 1.55MB/s]\n",
            "Generating train split: 100% 7473/7473 [00:00<00:00, 160584.64 examples/s]\n",
            "Generating test split: 100% 1319/1319 [00:00<00:00, 324836.30 examples/s]\n",
            "2024-04-03:20:21:07,957 INFO     [task.py:395] Building contexts for gsm8k on rank 0...\n",
            "100% 1319/1319 [00:09<00:00, 145.43it/s]\n",
            "2024-04-03:20:21:17,055 INFO     [evaluator.py:379] Running generate_until requests\n",
            "Running generate_until requests:   0% 0/1319 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1027 > 1024). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1339: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "Running generate_until requests:  13% 173/1319 [04:23<30:21,  1.59s/it]"
          ]
        }
      ],
      "source": [
        "!lm_eval \\\n",
        "  --model huggingface \\\n",
        "  --model_args pretrained=openai-community/gpt2,num_beams=1,temperature=0 \\\n",
        "  --tasks gsm8k \\\n",
        "  --batch_size 2 \\\n",
        "  --seed 42 \\\n",
        "  --device cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_n3MUHdHztq"
      },
      "source": [
        "2024-03-19 16:29:57\n",
        "\n",
        "Running generate_until requests: 100% 1319/1319 [29:32<00:00,  1.34s/it]\n",
        "\n",
        "huggingface (pretrained=openai-community/gpt2,num_beams=1,temperature=0), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSfUPLeNHPBx"
      },
      "source": [
        "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
        "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
        "|gsm8k|      3|strict-match    |     5|exact_match|0.0030|±  |0.0015|\n",
        "|     |       |flexible-extract|     5|exact_match|0.0068|±  |0.0023|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QUZABPuLqtV"
      },
      "source": [
        "temp, beam = (0, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-TJ-CCT9vg5",
        "outputId": "d5329f03-f61a-4581-b543-4e2a54bc5032"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-19 17:01:42.086431: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-19 17:01:42.086489: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-19 17:01:42.088050: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-19 17:01:43.385104: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-03-19:17:01:47,811 INFO     [__main__.py:251] Verbosity set to INFO\n",
            "2024-03-19:17:01:53,549 INFO     [__main__.py:335] Selected Tasks: ['gsm8k']\n",
            "2024-03-19:17:01:53,549 INFO     [__main__.py:336] Loading selected tasks...\n",
            "2024-03-19:17:01:53,551 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-03-19:17:01:53,594 INFO     [huggingface.py:163] Using device 'cuda'\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "2024-03-19:17:01:55,478 INFO     [huggingface.py:345] Loglikelihood prefix token id used in evaluation: 50256\n",
            "2024-03-19:17:01:58,787 INFO     [task.py:395] Building contexts for gsm8k on rank 0...\n",
            "100% 1319/1319 [00:08<00:00, 152.78it/s]\n",
            "2024-03-19:17:02:07,450 INFO     [evaluator.py:362] Running generate_until requests\n",
            "Running generate_until requests:   0% 0/1319 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1027 > 1024). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1339: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "Running generate_until requests: 100% 1319/1319 [31:07<00:00,  1.42s/it]\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "huggingface (pretrained=openai-community/gpt2,num_beams=2,temperature=0), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
            "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
            "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
            "|gsm8k|      3|strict-match    |     5|exact_match|0.0045|±  |0.0019|\n",
            "|     |       |flexible-extract|     5|exact_match|0.0190|±  |0.0038|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!lm_eval \\\n",
        "  --model huggingface \\\n",
        "  --model_args pretrained=openai-community/gpt2,num_beams=2,temperature=0 \\\n",
        "  --tasks gsm8k \\\n",
        "  --batch_size 2 \\\n",
        "  --device cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0h1R-jldMbtP"
      },
      "source": [
        "2024-03-19 17:01:42\n",
        "\n",
        "Running generate_until requests: 100% 1319/1319 [31:07<00:00,  1.42s/it]\n",
        "\n",
        "huggingface (pretrained=openai-community/gpt2,num_beams=2,temperature=0), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVOeUs6LMUSU"
      },
      "source": [
        "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
        "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
        "|gsm8k|      3|strict-match    |     5|exact_match|0.0045|±  |0.0019|\n",
        "|     |       |flexible-extract|     5|exact_match|0.0190|±  |0.0038|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04S7B_8qMrVQ"
      },
      "source": [
        "temp, beam = (0, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNhrwB9I9v-Z",
        "outputId": "1d1a74b6-89dd-4f11-80de-c19b22e34aa0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-19 18:34:21.327609: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-19 18:34:21.327652: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-19 18:34:21.329064: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-19 18:34:22.585751: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-03-19:18:34:26,778 INFO     [__main__.py:251] Verbosity set to INFO\n",
            "2024-03-19:18:34:32,115 INFO     [__main__.py:335] Selected Tasks: ['gsm8k']\n",
            "2024-03-19:18:34:32,115 INFO     [__main__.py:336] Loading selected tasks...\n",
            "2024-03-19:18:34:32,117 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-03-19:18:34:32,158 INFO     [huggingface.py:163] Using device 'cuda'\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "2024-03-19:18:34:33,968 INFO     [huggingface.py:345] Loglikelihood prefix token id used in evaluation: 50256\n",
            "2024-03-19:18:34:37,098 INFO     [task.py:395] Building contexts for gsm8k on rank 0...\n",
            "100% 1319/1319 [00:08<00:00, 159.26it/s]\n",
            "2024-03-19:18:34:45,410 INFO     [evaluator.py:362] Running generate_until requests\n",
            "Running generate_until requests:   0% 0/1319 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1027 > 1024). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1339: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "Running generate_until requests: 100% 1319/1319 [37:08<00:00,  1.69s/it]\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "huggingface (pretrained=openai-community/gpt2,num_beams=3,temperature=0), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
            "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
            "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
            "|gsm8k|      3|strict-match    |     5|exact_match|0.0038|±  |0.0017|\n",
            "|     |       |flexible-extract|     5|exact_match|0.0205|±  |0.0039|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!lm_eval \\\n",
        "  --model huggingface \\\n",
        "  --model_args pretrained=openai-community/gpt2,num_beams=3,temperature=0 \\\n",
        "  --tasks gsm8k \\\n",
        "  --batch_size 2 \\\n",
        "  --device cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LERPzk3M3Y4"
      },
      "source": [
        "2024-03-19 18:34:21\n",
        "\n",
        "Running generate_until requests: 100% 1319/1319 [37:08<00:00,  1.69s/it]\n",
        "\n",
        "huggingface (pretrained=openai-community/gpt2,num_beams=3,temperature=0), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qf9x02DM1KP"
      },
      "source": [
        "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
        "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
        "|gsm8k|      3|strict-match    |     5|exact_match|0.0038|±  |0.0017|\n",
        "|     |       |flexible-extract|     5|exact_match|0.0205|±  |0.0039|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl23M75aNPob"
      },
      "source": [
        "temp, beam = (0, 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoVP31yJ9waB",
        "outputId": "2893f358-4ce6-4a56-ddc4-5843d8d6e4e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-21 13:33:14.904145: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-21 13:33:14.904206: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-21 13:33:14.905637: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-21 13:33:16.253579: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-03-21:13:33:19,856 INFO     [__main__.py:251] Verbosity set to INFO\n",
            "2024-03-21:13:33:26,037 INFO     [__main__.py:335] Selected Tasks: ['gsm8k']\n",
            "2024-03-21:13:33:26,037 INFO     [__main__.py:336] Loading selected tasks...\n",
            "2024-03-21:13:33:26,039 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-03-21:13:33:26,082 INFO     [huggingface.py:163] Using device 'cuda'\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "2024-03-21:13:33:28,738 INFO     [task.py:395] Building contexts for gsm8k on rank 0...\n",
            "100% 1319/1319 [00:09<00:00, 139.41it/s]\n",
            "2024-03-21:13:33:38,232 INFO     [evaluator.py:362] Running generate_until requests\n",
            "Running generate_until requests:   0% 0/1319 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1027 > 1024). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1339: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "Running generate_until requests: 100% 1319/1319 [1:05:48<00:00,  2.99s/it]\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "huggingface (pretrained=openai-community/gpt2,num_beams=6,temperature=0), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
            "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
            "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
            "|gsm8k|      3|strict-match    |     5|exact_match|0.0030|±  |0.0015|\n",
            "|     |       |flexible-extract|     5|exact_match|0.0159|±  |0.0034|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!lm_eval \\\n",
        "  --model huggingface \\\n",
        "  --model_args pretrained=openai-community/gpt2,num_beams=6,temperature=0 \\\n",
        "  --tasks gsm8k \\\n",
        "  --batch_size 2 \\\n",
        "  --device cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unzNCaDKfKwz"
      },
      "source": [
        "2024-03-21 13:33:14\n",
        "\n",
        "Running generate_until requests: 100% 1319/1319 [1:05:48<00:00,  2.99s/it]\n",
        "\n",
        "huggingface (pretrained=openai-community/gpt2,num_beams=6,temperature=0), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOIiRxDMd6hu"
      },
      "source": [
        "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
        "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
        "|gsm8k|      3|strict-match    |     5|exact_match|0.0030|±  |0.0015|\n",
        "|     |       |flexible-extract|     5|exact_match|0.0159|±  |0.0034|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQCKKMBpNZp1"
      },
      "source": [
        "## Temperature = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHZDHGBj_uiZ"
      },
      "source": [
        "temp, beam = (0.1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrEkXxfDeF7o",
        "outputId": "b4d087ef-0e6f-43c1-e69a-a9c7cfbfe4b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-21 14:44:36.435819: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-21 14:44:36.435875: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-21 14:44:36.437376: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-21 14:44:37.756085: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-03-21:14:44:41,395 INFO     [__main__.py:251] Verbosity set to INFO\n",
            "2024-03-21:14:44:47,590 INFO     [__main__.py:335] Selected Tasks: ['gsm8k']\n",
            "2024-03-21:14:44:47,590 INFO     [__main__.py:336] Loading selected tasks...\n",
            "2024-03-21:14:44:47,592 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-03-21:14:44:47,637 INFO     [huggingface.py:163] Using device 'cuda'\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "2024-03-21:14:44:50,215 INFO     [task.py:395] Building contexts for gsm8k on rank 0...\n",
            "100% 1319/1319 [00:09<00:00, 142.33it/s]\n",
            "2024-03-21:14:44:59,514 INFO     [evaluator.py:362] Running generate_until requests\n",
            "Running generate_until requests:   0% 0/1319 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1027 > 1024). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1339: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "Running generate_until requests: 100% 1319/1319 [29:34<00:00,  1.35s/it]\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "huggingface (pretrained=openai-community/gpt2,num_beams=1,temperature=0.1), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
            "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
            "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
            "|gsm8k|      3|strict-match    |     5|exact_match|0.0030|±  |0.0015|\n",
            "|     |       |flexible-extract|     5|exact_match|0.0068|±  |0.0023|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!lm_eval \\\n",
        "  --model huggingface \\\n",
        "  --model_args pretrained=openai-community/gpt2,num_beams=1,temperature=0.1 \\\n",
        "  --tasks gsm8k \\\n",
        "  --batch_size 2 \\\n",
        "  --device cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3yFPt5mt7c-"
      },
      "source": [
        "2024-03-21 14:44:36\n",
        "\n",
        "Running generate_until requests: 100% 1319/1319 [29:34<00:00,  1.35s/it]\n",
        "\n",
        "huggingface (pretrained=openai-community/gpt2,num_beams=1,temperature=0.1), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pzkj8RUht4e5"
      },
      "source": [
        "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
        "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
        "|gsm8k|      3|strict-match    |     5|exact_match|0.0030|±  |0.0015|\n",
        "|     |       |flexible-extract|     5|exact_match|0.0068|±  |0.0023|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDWEzVuSfr1D"
      },
      "source": [
        "temp, beam = (0.1, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wShB4wSwf1O6",
        "outputId": "82557548-ac4d-4b05-ec4b-7386af9ee4d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-21 15:14:53.321352: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-21 15:14:53.321417: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-21 15:14:53.322917: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-21 15:14:54.717216: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-03-21:15:14:58,425 INFO     [__main__.py:251] Verbosity set to INFO\n",
            "2024-03-21:15:15:04,598 INFO     [__main__.py:335] Selected Tasks: ['gsm8k']\n",
            "2024-03-21:15:15:04,598 INFO     [__main__.py:336] Loading selected tasks...\n",
            "2024-03-21:15:15:04,600 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-03-21:15:15:04,645 INFO     [huggingface.py:163] Using device 'cuda'\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "2024-03-21:15:15:07,439 INFO     [task.py:395] Building contexts for gsm8k on rank 0...\n",
            "100% 1319/1319 [00:09<00:00, 140.92it/s]\n",
            "2024-03-21:15:15:16,832 INFO     [evaluator.py:362] Running generate_until requests\n",
            "Running generate_until requests:   0% 0/1319 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1027 > 1024). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1339: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "Running generate_until requests: 100% 1319/1319 [32:53<00:00,  1.50s/it]\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "huggingface (pretrained=openai-community/gpt2,num_beams=2,temperature=0.1), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
            "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
            "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
            "|gsm8k|      3|strict-match    |     5|exact_match|0.0045|±  |0.0019|\n",
            "|     |       |flexible-extract|     5|exact_match|0.0190|±  |0.0038|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!lm_eval \\\n",
        "  --model huggingface \\\n",
        "  --model_args pretrained=openai-community/gpt2,num_beams=2,temperature=0.1 \\\n",
        "  --tasks gsm8k \\\n",
        "  --batch_size 2 \\\n",
        "  --device cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKHHyXJNthny"
      },
      "source": [
        "2024-03-21 15:14:53\n",
        "\n",
        "Running generate_until requests: 100% 1319/1319 [32:53<00:00,  1.50s/it]\n",
        "\n",
        "huggingface (pretrained=openai-community/gpt2,num_beams=2,temperature=0.1), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFSJgRKRtr0B"
      },
      "source": [
        "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
        "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
        "|gsm8k|      3|strict-match    |     5|exact_match|0.0045|±  |0.0019|\n",
        "|     |       |flexible-extract|     5|exact_match|0.0190|±  |0.0038|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiNub5hmfwT6"
      },
      "source": [
        "temp, beam = (0.1, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9O_f4v18f5i8",
        "outputId": "712e81cf-2594-4487-d110-93b0f55a151b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-21 15:48:28.933116: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-21 15:48:28.933172: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-21 15:48:28.934662: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-21 15:48:30.330856: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-03-21:15:48:34,063 INFO     [__main__.py:251] Verbosity set to INFO\n",
            "2024-03-21:15:48:40,204 INFO     [__main__.py:335] Selected Tasks: ['gsm8k']\n",
            "2024-03-21:15:48:40,204 INFO     [__main__.py:336] Loading selected tasks...\n",
            "2024-03-21:15:48:40,206 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-03-21:15:48:40,250 INFO     [huggingface.py:163] Using device 'cuda'\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "2024-03-21:15:48:42,911 INFO     [task.py:395] Building contexts for gsm8k on rank 0...\n",
            "100% 1319/1319 [00:09<00:00, 138.80it/s]\n",
            "2024-03-21:15:48:52,446 INFO     [evaluator.py:362] Running generate_until requests\n",
            "Running generate_until requests:   0% 0/1319 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1027 > 1024). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1339: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "Running generate_until requests: 100% 1319/1319 [40:43<00:00,  1.85s/it]\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "huggingface (pretrained=openai-community/gpt2,num_beams=3,temperature=0.1), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
            "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
            "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
            "|gsm8k|      3|strict-match    |     5|exact_match|0.0038|±  |0.0017|\n",
            "|     |       |flexible-extract|     5|exact_match|0.0205|±  |0.0039|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!lm_eval \\\n",
        "  --model huggingface \\\n",
        "  --model_args pretrained=openai-community/gpt2,num_beams=3,temperature=0.1 \\\n",
        "  --tasks gsm8k \\\n",
        "  --batch_size 2 \\\n",
        "  --device cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NCJqNxoOqn3"
      },
      "source": [
        "2024-03-21 15:48:28\n",
        "\n",
        "Running generate_until requests: 100% 1319/1319 [40:43<00:00,  1.85s/it]\n",
        "\n",
        "huggingface (pretrained=openai-community/gpt2,num_beams=3,temperature=0.1), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0KVxbgwOolt"
      },
      "source": [
        "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
        "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
        "|gsm8k|      3|strict-match    |     5|exact_match|0.0038|±  |0.0017|\n",
        "|     |       |flexible-extract|     5|exact_match|0.0205|±  |0.0039|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5e5vIFEfyXs"
      },
      "source": [
        "temp, beam = (0.1, 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcwAUjHPf8Q5",
        "outputId": "d61b76b3-bb47-4fcf-912f-82121338d4df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-21 16:29:54.640104: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-21 16:29:54.640163: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-21 16:29:54.641678: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-21 16:29:56.012159: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-03-21:16:29:59,739 INFO     [__main__.py:251] Verbosity set to INFO\n",
            "2024-03-21:16:30:05,857 INFO     [__main__.py:335] Selected Tasks: ['gsm8k']\n",
            "2024-03-21:16:30:05,857 INFO     [__main__.py:336] Loading selected tasks...\n",
            "2024-03-21:16:30:05,859 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-03-21:16:30:05,904 INFO     [huggingface.py:163] Using device 'cuda'\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "2024-03-21:16:30:08,597 INFO     [task.py:395] Building contexts for gsm8k on rank 0...\n",
            "100% 1319/1319 [00:09<00:00, 141.06it/s]\n",
            "2024-03-21:16:30:17,981 INFO     [evaluator.py:362] Running generate_until requests\n",
            "Running generate_until requests:   0% 0/1319 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1027 > 1024). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1339: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "Running generate_until requests: 100% 1319/1319 [1:06:00<00:00,  3.00s/it]\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "huggingface (pretrained=openai-community/gpt2,num_beams=6,temperature=0.1), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
            "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
            "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
            "|gsm8k|      3|strict-match    |     5|exact_match|0.0030|±  |0.0015|\n",
            "|     |       |flexible-extract|     5|exact_match|0.0159|±  |0.0034|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!lm_eval \\\n",
        "  --model huggingface \\\n",
        "  --model_args pretrained=openai-community/gpt2,num_beams=6,temperature=0.1 \\\n",
        "  --tasks gsm8k \\\n",
        "  --batch_size 2 \\\n",
        "  --device cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6u56ELGO9t8"
      },
      "source": [
        "2024-03-21 16:29:54\n",
        "\n",
        "Running generate_until requests: 100% 1319/1319 [1:06:00<00:00,  3.00s/it]\n",
        "\n",
        "huggingface (pretrained=openai-community/gpt2,num_beams=6,temperature=0.1), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UxNCYX9O4cn"
      },
      "source": [
        "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
        "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
        "|gsm8k|      3|strict-match    |     5|exact_match|0.0030|±  |0.0015|\n",
        "|     |       |flexible-extract|     5|exact_match|0.0159|±  |0.0034|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pB9TcmAQvng"
      },
      "source": [
        "## Temperature = 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UOg0YXpMI1C"
      },
      "source": [
        "temp, beam = (0.5, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Swa-e3rKL6IK",
        "outputId": "6186bb0f-a126-4f42-b354-68fdf9ff094a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-23 12:50:55.356962: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-23 12:50:55.357016: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-23 12:50:55.358398: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-23 12:50:56.542047: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading builder script: 100% 5.67k/5.67k [00:00<00:00, 20.9MB/s]\n",
            "2024-03-23:12:51:02,354 INFO     [__main__.py:251] Verbosity set to INFO\n",
            "2024-03-23:12:51:08,083 INFO     [__main__.py:335] Selected Tasks: ['gsm8k']\n",
            "2024-03-23:12:51:08,089 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-03-23:12:51:08,089 INFO     [evaluator.py:177] Initializing huggingface model, with arguments: {'pretrained': 'openai-community/gpt2', 'num_beams': 1, 'temperature': 0.5}\n",
            "2024-03-23:12:51:08,156 INFO     [huggingface.py:163] Using device 'cuda'\n",
            "config.json: 100% 665/665 [00:00<00:00, 4.55MB/s]\n",
            "model.safetensors: 100% 548M/548M [00:01<00:00, 365MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "generation_config.json: 100% 124/124 [00:00<00:00, 832kB/s]\n",
            "tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 161kB/s]\n",
            "vocab.json: 100% 1.04M/1.04M [00:00<00:00, 2.65MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 2.36MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 3.45MB/s]\n",
            "Downloading readme: 100% 7.94k/7.94k [00:00<00:00, 28.1MB/s]\n",
            "Downloading data: 100% 2.31M/2.31M [00:00<00:00, 12.1MB/s]\n",
            "Downloading data: 100% 419k/419k [00:00<00:00, 3.12MB/s]\n",
            "Generating train split: 100% 7473/7473 [00:00<00:00, 172657.30 examples/s]\n",
            "Generating test split: 100% 1319/1319 [00:00<00:00, 297354.85 examples/s]\n",
            "2024-03-23:12:51:18,138 INFO     [task.py:395] Building contexts for gsm8k on rank 0...\n",
            "100% 1319/1319 [00:08<00:00, 154.43it/s]\n",
            "2024-03-23:12:51:26,707 INFO     [evaluator.py:379] Running generate_until requests\n",
            "Running generate_until requests:   0% 0/1319 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1027 > 1024). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1339: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "Running generate_until requests: 100% 1319/1319 [27:36<00:00,  1.26s/it]\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "huggingface (pretrained=openai-community/gpt2,num_beams=1,temperature=0.5), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
            "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
            "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
            "|gsm8k|      3|strict-match    |     5|exact_match|0.0030|±  |0.0015|\n",
            "|     |       |flexible-extract|     5|exact_match|0.0068|±  |0.0023|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!lm_eval \\\n",
        "  --model huggingface \\\n",
        "  --model_args pretrained=openai-community/gpt2,num_beams=1,temperature=0.5 \\\n",
        "  --tasks gsm8k \\\n",
        "  --batch_size 2 \\\n",
        "  --device cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1umoJWihTFL"
      },
      "source": [
        "Running generate_until requests: 100% 1319/1319 [27:36<00:00,  1.26s/it]\n",
        "\n",
        "huggingface (pretrained=openai-community/gpt2,num_beams=1,temperature=0.5), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
        "\n",
        "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
        "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
        "|gsm8k|      3|strict-match    |     5|exact_match|0.0030|±  |0.0015|\n",
        "|     |       |flexible-extract|     5|exact_match|0.0068|±  |0.0023|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_i4BEG6WMPnD"
      },
      "source": [
        "temp, beam = (0.5, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sGsC_v-MXTf",
        "outputId": "e1a75156-19f1-4349-b174-e866ba349aaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-23 13:34:46.652739: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-23 13:34:46.652784: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-23 13:34:46.654161: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-23 13:34:47.861442: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-03-23:13:34:51,537 INFO     [__main__.py:251] Verbosity set to INFO\n",
            "2024-03-23:13:34:57,214 INFO     [__main__.py:335] Selected Tasks: ['gsm8k']\n",
            "2024-03-23:13:34:57,215 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-03-23:13:34:57,216 INFO     [evaluator.py:177] Initializing huggingface model, with arguments: {'pretrained': 'openai-community/gpt2', 'num_beams': 2, 'temperature': 0.5}\n",
            "2024-03-23:13:34:57,258 INFO     [huggingface.py:163] Using device 'cuda'\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "2024-03-23:13:35:00,749 INFO     [task.py:395] Building contexts for gsm8k on rank 0...\n",
            "100% 1319/1319 [00:08<00:00, 158.38it/s]\n",
            "2024-03-23:13:35:09,107 INFO     [evaluator.py:379] Running generate_until requests\n",
            "Running generate_until requests:   0% 0/1319 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1027 > 1024). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1339: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "Running generate_until requests: 100% 1319/1319 [30:43<00:00,  1.40s/it]\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "huggingface (pretrained=openai-community/gpt2,num_beams=2,temperature=0.5), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
            "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
            "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
            "|gsm8k|      3|strict-match    |     5|exact_match|0.0045|±  |0.0019|\n",
            "|     |       |flexible-extract|     5|exact_match|0.0190|±  |0.0038|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!lm_eval \\\n",
        "  --model huggingface \\\n",
        "  --model_args pretrained=openai-community/gpt2,num_beams=2,temperature=0.5 \\\n",
        "  --tasks gsm8k \\\n",
        "  --batch_size 2 \\\n",
        "  --device cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeVWYu5ArODb"
      },
      "source": [
        "Running generate_until requests: 100% 1319/1319 [30:43<00:00,  1.40s/it]\n",
        "\n",
        "huggingface (pretrained=openai-community/gpt2,num_beams=2,temperature=0.5), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
        "\n",
        "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
        "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
        "|gsm8k|      3|strict-match    |     5|exact_match|0.0045|±  |0.0019|\n",
        "|     |       |flexible-extract|     5|exact_match|0.0190|±  |0.0038|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40lNiyp7Mfe2"
      },
      "source": [
        "temp, beam = (0.5, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eo-gmYalMaA7",
        "outputId": "c471c60d-5e93-4c87-a495-f6e9157164a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-23 14:17:59.375069: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-23 14:17:59.375118: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-23 14:17:59.376510: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-23 14:18:00.576858: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-03-23:14:18:04,324 INFO     [__main__.py:251] Verbosity set to INFO\n",
            "2024-03-23:14:18:10,013 INFO     [__main__.py:335] Selected Tasks: ['gsm8k']\n",
            "2024-03-23:14:18:10,015 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-03-23:14:18:10,015 INFO     [evaluator.py:177] Initializing huggingface model, with arguments: {'pretrained': 'openai-community/gpt2', 'num_beams': 3, 'temperature': 0.5}\n",
            "2024-03-23:14:18:10,056 INFO     [huggingface.py:163] Using device 'cuda'\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "2024-03-23:14:18:13,455 INFO     [task.py:395] Building contexts for gsm8k on rank 0...\n",
            "100% 1319/1319 [00:08<00:00, 158.07it/s]\n",
            "2024-03-23:14:18:21,829 INFO     [evaluator.py:379] Running generate_until requests\n",
            "Running generate_until requests:   0% 0/1319 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1027 > 1024). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1339: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "Running generate_until requests: 100% 1319/1319 [37:44<00:00,  1.72s/it]\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "huggingface (pretrained=openai-community/gpt2,num_beams=3,temperature=0.5), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
            "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
            "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
            "|gsm8k|      3|strict-match    |     5|exact_match|0.0038|±  |0.0017|\n",
            "|     |       |flexible-extract|     5|exact_match|0.0205|±  |0.0039|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!lm_eval \\\n",
        "  --model huggingface \\\n",
        "  --model_args pretrained=openai-community/gpt2,num_beams=3,temperature=0.5 \\\n",
        "  --tasks gsm8k \\\n",
        "  --batch_size 2 \\\n",
        "  --device cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo_cLsP85pte"
      },
      "source": [
        "Running generate_until requests: 100% 1319/1319 [37:44<00:00,  1.72s/it]\n",
        "\n",
        "huggingface (pretrained=openai-community/gpt2,num_beams=3,temperature=0.5), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
        "\n",
        "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
        "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
        "|gsm8k|      3|strict-match    |     5|exact_match|0.0038|±  |0.0017|\n",
        "|     |       |flexible-extract|     5|exact_match|0.0205|±  |0.0039|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xla1zUCLNGRY"
      },
      "source": [
        "temp, beam = (0.5, 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZLlQsGwNJpu",
        "outputId": "6a330975-0277-4339-bea1-733a3501b3b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-23 15:20:54.238701: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-23 15:20:54.238756: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-23 15:20:54.240248: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-23 15:20:55.440659: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-03-23:15:20:59,157 INFO     [__main__.py:251] Verbosity set to INFO\n",
            "2024-03-23:15:21:04,904 INFO     [__main__.py:335] Selected Tasks: ['gsm8k']\n",
            "2024-03-23:15:21:04,905 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-03-23:15:21:04,906 INFO     [evaluator.py:177] Initializing huggingface model, with arguments: {'pretrained': 'openai-community/gpt2', 'num_beams': 6, 'temperature': 0.5}\n",
            "2024-03-23:15:21:04,950 INFO     [huggingface.py:163] Using device 'cuda'\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "2024-03-23:15:21:08,343 INFO     [task.py:395] Building contexts for gsm8k on rank 0...\n",
            "100% 1319/1319 [00:08<00:00, 154.67it/s]\n",
            "2024-03-23:15:21:16,898 INFO     [evaluator.py:379] Running generate_until requests\n",
            "Running generate_until requests:   0% 0/1319 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1027 > 1024). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1339: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "Running generate_until requests: 100% 1319/1319 [1:04:01<00:00,  2.91s/it]\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "huggingface (pretrained=openai-community/gpt2,num_beams=6,temperature=0.5), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
            "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
            "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
            "|gsm8k|      3|strict-match    |     5|exact_match|0.0030|±  |0.0015|\n",
            "|     |       |flexible-extract|     5|exact_match|0.0159|±  |0.0034|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!lm_eval \\\n",
        "  --model huggingface \\\n",
        "  --model_args pretrained=openai-community/gpt2,num_beams=6,temperature=0.5 \\\n",
        "  --tasks gsm8k \\\n",
        "  --batch_size 2 \\\n",
        "  --device cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVtDtOZkNtXO"
      },
      "source": [
        "Running generate_until requests: 100% 1319/1319 [1:04:01<00:00,  2.91s/it]\n",
        "\n",
        "huggingface (pretrained=openai-community/gpt2,num_beams=6,temperature=0.5), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
        "\n",
        "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
        "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
        "|gsm8k|      3|strict-match    |     5|exact_match|0.0030|±  |0.0015|\n",
        "|     |       |flexible-extract|     5|exact_match|0.0159|±  |0.0034|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhRINAPKQy4m"
      },
      "source": [
        "## Temperature = 0.7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfchLcfjNNTz"
      },
      "source": [
        "temp, beam = (0.7, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceezg_MkNS54",
        "outputId": "0fa3b97a-0937-460d-817c-35905f8dc2f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-23 16:48:28.857035: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-23 16:48:28.857086: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-23 16:48:28.858604: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-23 16:48:30.067460: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-03-23:16:48:33,735 INFO     [__main__.py:251] Verbosity set to INFO\n",
            "2024-03-23:16:48:39,368 INFO     [__main__.py:335] Selected Tasks: ['gsm8k']\n",
            "2024-03-23:16:48:39,370 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-03-23:16:48:39,370 INFO     [evaluator.py:177] Initializing huggingface model, with arguments: {'pretrained': 'openai-community/gpt2', 'num_beams': 1, 'temperature': 0.7}\n",
            "2024-03-23:16:48:39,414 INFO     [huggingface.py:163] Using device 'cuda'\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "2024-03-23:16:48:42,717 INFO     [task.py:395] Building contexts for gsm8k on rank 0...\n",
            "100% 1319/1319 [00:08<00:00, 155.17it/s]\n",
            "2024-03-23:16:48:51,247 INFO     [evaluator.py:379] Running generate_until requests\n",
            "Running generate_until requests:   0% 0/1319 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1027 > 1024). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1339: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "Running generate_until requests: 100% 1319/1319 [27:47<00:00,  1.26s/it]\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "huggingface (pretrained=openai-community/gpt2,num_beams=1,temperature=0.7), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
            "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
            "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
            "|gsm8k|      3|strict-match    |     5|exact_match|0.0030|±  |0.0015|\n",
            "|     |       |flexible-extract|     5|exact_match|0.0068|±  |0.0023|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!lm_eval \\\n",
        "  --model huggingface \\\n",
        "  --model_args pretrained=openai-community/gpt2,num_beams=1,temperature=0.7 \\\n",
        "  --tasks gsm8k \\\n",
        "  --batch_size 2 \\\n",
        "  --device cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vr9mannUaA0"
      },
      "source": [
        "Running generate_until requests: 100% 1319/1319 [27:47<00:00,  1.26s/it]\n",
        "\n",
        "huggingface (pretrained=openai-community/gpt2,num_beams=1,temperature=0.7), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
        "\n",
        "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
        "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
        "|gsm8k|      3|strict-match    |     5|exact_match|0.0030|±  |0.0015|\n",
        "|     |       |flexible-extract|     5|exact_match|0.0068|±  |0.0023|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bu9MEx1FNZoS"
      },
      "source": [
        "temp, beam = (0.7, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGDa6gjZNVnz",
        "outputId": "20f9dd35-0a89-49c1-911c-5f5835f01f16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-23 17:17:47.721964: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-23 17:17:47.722012: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-23 17:17:47.723487: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-23 17:17:48.921435: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-03-23:17:17:52,633 INFO     [__main__.py:251] Verbosity set to INFO\n",
            "2024-03-23:17:17:58,210 INFO     [__main__.py:335] Selected Tasks: ['gsm8k']\n",
            "2024-03-23:17:17:58,212 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-03-23:17:17:58,212 INFO     [evaluator.py:177] Initializing huggingface model, with arguments: {'pretrained': 'openai-community/gpt2', 'num_beams': 2, 'temperature': 0.7}\n",
            "2024-03-23:17:17:58,254 INFO     [huggingface.py:163] Using device 'cuda'\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "2024-03-23:17:18:01,659 INFO     [task.py:395] Building contexts for gsm8k on rank 0...\n",
            "100% 1319/1319 [00:08<00:00, 159.36it/s]\n",
            "2024-03-23:17:18:09,968 INFO     [evaluator.py:379] Running generate_until requests\n",
            "Running generate_until requests:   0% 0/1319 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1027 > 1024). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1339: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "Running generate_until requests: 100% 1319/1319 [31:07<00:00,  1.42s/it]\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "huggingface (pretrained=openai-community/gpt2,num_beams=2,temperature=0.7), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
            "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
            "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
            "|gsm8k|      3|strict-match    |     5|exact_match|0.0045|±  |0.0019|\n",
            "|     |       |flexible-extract|     5|exact_match|0.0190|±  |0.0038|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!lm_eval \\\n",
        "  --model huggingface \\\n",
        "  --model_args pretrained=openai-community/gpt2,num_beams=2,temperature=0.7 \\\n",
        "  --tasks gsm8k \\\n",
        "  --batch_size 2 \\\n",
        "  --device cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM-bKGN_cTBD"
      },
      "source": [
        "Running generate_until requests: 100% 1319/1319 [31:07<00:00,  1.42s/it]\n",
        "\n",
        "huggingface (pretrained=openai-community/gpt2,num_beams=2,temperature=0.7), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
        "\n",
        "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
        "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
        "|gsm8k|      3|strict-match    |     5|exact_match|0.0045|±  |0.0019|\n",
        "|     |       |flexible-extract|     5|exact_match|0.0190|±  |0.0038|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VEqVltKNjGj"
      },
      "source": [
        "temp, beam = (0.7, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Amj9Bwm-NWWI",
        "outputId": "a0521bd9-b9bb-407e-82ac-52818a6f4c02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-27 19:40:52.660946: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-27 19:40:52.661012: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-27 19:40:52.662508: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-27 19:40:53.904977: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading builder script: 100% 5.67k/5.67k [00:00<00:00, 18.8MB/s]\n",
            "2024-03-27:19:40:59,844 INFO     [__main__.py:251] Verbosity set to INFO\n",
            "2024-03-27:19:41:05,351 INFO     [__main__.py:335] Selected Tasks: ['gsm8k']\n",
            "2024-03-27:19:41:05,356 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-03-27:19:41:05,356 INFO     [evaluator.py:177] Initializing huggingface model, with arguments: {'pretrained': 'openai-community/gpt2', 'num_beams': 3, 'temperature': 0.7}\n",
            "2024-03-27:19:41:05,429 INFO     [huggingface.py:163] Using device 'cuda'\n",
            "config.json: 100% 665/665 [00:00<00:00, 3.74MB/s]\n",
            "model.safetensors: 100% 548M/548M [00:01<00:00, 355MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "generation_config.json: 100% 124/124 [00:00<00:00, 795kB/s]\n",
            "tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 132kB/s]\n",
            "vocab.json: 100% 1.04M/1.04M [00:00<00:00, 3.21MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 5.53MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 15.0MB/s]\n",
            "Downloading readme: 100% 7.94k/7.94k [00:00<00:00, 29.3MB/s]\n",
            "Downloading data: 100% 2.31M/2.31M [00:00<00:00, 18.3MB/s]\n",
            "Downloading data: 100% 419k/419k [00:00<00:00, 3.81MB/s]\n",
            "Generating train split: 100% 7473/7473 [00:00<00:00, 159245.00 examples/s]\n",
            "Generating test split: 100% 1319/1319 [00:00<00:00, 320954.17 examples/s]\n",
            "2024-03-27:19:41:14,248 INFO     [task.py:395] Building contexts for gsm8k on rank 0...\n",
            "100% 1319/1319 [00:08<00:00, 162.66it/s]\n",
            "2024-03-27:19:41:22,385 INFO     [evaluator.py:379] Running generate_until requests\n",
            "Running generate_until requests:   0% 0/1319 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1027 > 1024). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1339: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "Running generate_until requests: 100% 1319/1319 [37:05<00:00,  1.69s/it]\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "huggingface (pretrained=openai-community/gpt2,num_beams=3,temperature=0.7), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
            "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
            "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
            "|gsm8k|      3|strict-match    |     5|exact_match|0.0038|±  |0.0017|\n",
            "|     |       |flexible-extract|     5|exact_match|0.0205|±  |0.0039|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!lm_eval \\\n",
        "  --model huggingface \\\n",
        "  --model_args pretrained=openai-community/gpt2,num_beams=3,temperature=0.7 \\\n",
        "  --tasks gsm8k \\\n",
        "  --batch_size 2 \\\n",
        "  --device cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAaHp5668tlE"
      },
      "source": [
        "Running generate_until requests: 100% 1319/1319 [37:05<00:00,  1.69s/it]\n",
        "\n",
        "huggingface (pretrained=openai-community/gpt2,num_beams=3,temperature=0.7), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
        "\n",
        "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
        "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
        "|gsm8k|      3|strict-match    |     5|exact_match|0.0038|±  |0.0017|\n",
        "|     |       |flexible-extract|     5|exact_match|0.0205|±  |0.0039|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RozdGNd1NmsQ"
      },
      "source": [
        "temp, beam = (0.7, 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArCed5FUNW_j",
        "outputId": "afae4061-2a37-4d42-e39b-274d0974b249"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-28 07:27:18.933719: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-28 07:27:18.933779: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-28 07:27:18.935249: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-28 07:27:20.091368: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading builder script: 100% 5.67k/5.67k [00:00<00:00, 24.6MB/s]\n",
            "2024-03-28:07:27:25,359 INFO     [__main__.py:251] Verbosity set to INFO\n",
            "2024-03-28:07:27:30,873 INFO     [__main__.py:335] Selected Tasks: ['gsm8k']\n",
            "2024-03-28:07:27:30,877 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-03-28:07:27:30,878 INFO     [evaluator.py:177] Initializing huggingface model, with arguments: {'pretrained': 'openai-community/gpt2', 'num_beams': 6, 'temperature': 0.7}\n",
            "2024-03-28:07:27:30,942 INFO     [huggingface.py:163] Using device 'cuda'\n",
            "config.json: 100% 665/665 [00:00<00:00, 3.68MB/s]\n",
            "model.safetensors: 100% 548M/548M [00:01<00:00, 385MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "generation_config.json: 100% 124/124 [00:00<00:00, 690kB/s]\n",
            "tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 172kB/s]\n",
            "vocab.json: 100% 1.04M/1.04M [00:02<00:00, 506kB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 1.45MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:01<00:00, 700kB/s]\n",
            "Downloading readme: 100% 7.94k/7.94k [00:00<00:00, 30.8MB/s]\n",
            "Downloading data: 100% 2.31M/2.31M [00:00<00:00, 7.96MB/s]\n",
            "Downloading data: 100% 419k/419k [00:00<00:00, 3.73MB/s]\n",
            "Generating train split: 100% 7473/7473 [00:00<00:00, 201014.78 examples/s]\n",
            "Generating test split: 100% 1319/1319 [00:00<00:00, 364517.82 examples/s]\n",
            "2024-03-28:07:27:44,853 INFO     [task.py:395] Building contexts for gsm8k on rank 0...\n",
            "100% 1319/1319 [00:08<00:00, 164.58it/s]\n",
            "2024-03-28:07:27:52,895 INFO     [evaluator.py:379] Running generate_until requests\n",
            "Running generate_until requests:   0% 0/1319 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1027 > 1024). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1339: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "Running generate_until requests: 100% 1319/1319 [1:04:51<00:00,  2.95s/it]\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "huggingface (pretrained=openai-community/gpt2,num_beams=6,temperature=0.7), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
            "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
            "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
            "|gsm8k|      3|strict-match    |     5|exact_match|0.0030|±  |0.0015|\n",
            "|     |       |flexible-extract|     5|exact_match|0.0159|±  |0.0034|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!lm_eval \\\n",
        "  --model huggingface \\\n",
        "  --model_args pretrained=openai-community/gpt2,num_beams=6,temperature=0.7 \\\n",
        "  --tasks gsm8k \\\n",
        "  --batch_size 2 \\\n",
        "  --device cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVbNSOwTOwy-"
      },
      "source": [
        "Running generate_until requests: 100% 1319/1319 [1:04:51<00:00,  2.95s/it]\n",
        "\n",
        "huggingface (pretrained=openai-community/gpt2,num_beams=6,temperature=0.7), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
        "\n",
        "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
        "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
        "|gsm8k|      3|strict-match    |     5|exact_match|0.0030|±  |0.0015|\n",
        "|     |       |flexible-extract|     5|exact_match|0.0159|±  |0.0034|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaeNiLrnQ1MR"
      },
      "source": [
        "## Temperature = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPeDZQY6Vx6H"
      },
      "source": [
        "temp, beam = (1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCZ-AJyUVjL2",
        "outputId": "8b4c5918-2255-4f5d-b11a-725b9cf63c0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-28 08:44:08.637583: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-28 08:44:08.637638: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-28 08:44:08.639015: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-28 08:44:09.828607: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-03-28:08:44:13,442 INFO     [__main__.py:251] Verbosity set to INFO\n",
            "2024-03-28:08:44:19,036 INFO     [__main__.py:335] Selected Tasks: ['gsm8k']\n",
            "2024-03-28:08:44:19,037 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-03-28:08:44:19,037 INFO     [evaluator.py:177] Initializing huggingface model, with arguments: {'pretrained': 'openai-community/gpt2', 'num_beams': 1, 'temperature': 1}\n",
            "2024-03-28:08:44:19,079 INFO     [huggingface.py:163] Using device 'cuda'\n",
            "2024-03-28:08:44:22,596 INFO     [task.py:395] Building contexts for gsm8k on rank 0...\n",
            "100% 1319/1319 [00:08<00:00, 160.87it/s]\n",
            "2024-03-28:08:44:30,824 INFO     [evaluator.py:379] Running generate_until requests\n",
            "Running generate_until requests:   0% 0/1319 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1027 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Running generate_until requests: 100% 1319/1319 [27:10<00:00,  1.24s/it]\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "huggingface (pretrained=openai-community/gpt2,num_beams=1,temperature=1), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
            "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
            "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
            "|gsm8k|      3|strict-match    |     5|exact_match|0.0030|±  |0.0015|\n",
            "|     |       |flexible-extract|     5|exact_match|0.0068|±  |0.0023|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!lm_eval \\\n",
        "  --model huggingface \\\n",
        "  --model_args pretrained=openai-community/gpt2,num_beams=1,temperature=1 \\\n",
        "  --tasks gsm8k \\\n",
        "  --batch_size 2 \\\n",
        "  --device cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jluNqWouWCay"
      },
      "source": [
        "Running generate_until requests: 100% 1319/1319 [27:10<00:00,  1.24s/it]\n",
        "\n",
        "huggingface (pretrained=openai-community/gpt2,num_beams=1,temperature=1), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
        "\n",
        "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
        "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
        "|gsm8k|      3|strict-match    |     5|exact_match|0.0030|±  |0.0015|\n",
        "|     |       |flexible-extract|     5|exact_match|0.0068|±  |0.0023|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBcfjVylV1hT"
      },
      "source": [
        "temp, beam = (1, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMaFnAlZVqr0",
        "outputId": "f0d24ca0-41b2-467e-a649-a54e19e7f6d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-28 09:15:49.623200: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-28 09:15:49.623255: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-28 09:15:49.624669: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-28 09:15:50.814603: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-03-28:09:15:54,362 INFO     [__main__.py:251] Verbosity set to INFO\n",
            "2024-03-28:09:15:59,891 INFO     [__main__.py:335] Selected Tasks: ['gsm8k']\n",
            "2024-03-28:09:15:59,892 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-03-28:09:15:59,893 INFO     [evaluator.py:177] Initializing huggingface model, with arguments: {'pretrained': 'openai-community/gpt2', 'num_beams': 2, 'temperature': 1}\n",
            "2024-03-28:09:15:59,932 INFO     [huggingface.py:163] Using device 'cuda'\n",
            "2024-03-28:09:16:03,183 INFO     [task.py:395] Building contexts for gsm8k on rank 0...\n",
            "100% 1319/1319 [00:08<00:00, 160.43it/s]\n",
            "2024-03-28:09:16:11,433 INFO     [evaluator.py:379] Running generate_until requests\n",
            "Running generate_until requests:   0% 0/1319 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1027 > 1024). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1339: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "Running generate_until requests: 100% 1319/1319 [30:14<00:00,  1.38s/it]\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "huggingface (pretrained=openai-community/gpt2,num_beams=2,temperature=1), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
            "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
            "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
            "|gsm8k|      3|strict-match    |     5|exact_match|0.0045|±  |0.0019|\n",
            "|     |       |flexible-extract|     5|exact_match|0.0190|±  |0.0038|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!lm_eval \\\n",
        "  --model huggingface \\\n",
        "  --model_args pretrained=openai-community/gpt2,num_beams=2,temperature=1 \\\n",
        "  --tasks gsm8k \\\n",
        "  --batch_size 2 \\\n",
        "  --device cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSaTdJlthBjR"
      },
      "source": [
        "Running generate_until requests: 100% 1319/1319 [30:14<00:00,  1.38s/it]\n",
        "\n",
        "huggingface (pretrained=openai-community/gpt2,num_beams=2,temperature=1), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
        "\n",
        "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
        "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
        "|gsm8k|      3|strict-match    |     5|exact_match|0.0045|±  |0.0019|\n",
        "|     |       |flexible-extract|     5|exact_match|0.0190|±  |0.0038|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvaV97ZZV3g6"
      },
      "source": [
        "temp, beam = (1, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXVLUM6YVsGg",
        "outputId": "559e1619-2b13-4f82-ffed-6b6f363b936f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-28 10:03:51.673458: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-28 10:03:51.673506: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-28 10:03:51.674927: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-28 10:03:52.820977: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-03-28:10:03:56,320 INFO     [__main__.py:251] Verbosity set to INFO\n",
            "2024-03-28:10:04:01,857 INFO     [__main__.py:335] Selected Tasks: ['gsm8k']\n",
            "2024-03-28:10:04:01,859 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-03-28:10:04:01,859 INFO     [evaluator.py:177] Initializing huggingface model, with arguments: {'pretrained': 'openai-community/gpt2', 'num_beams': 3, 'temperature': 1}\n",
            "2024-03-28:10:04:01,899 INFO     [huggingface.py:163] Using device 'cuda'\n",
            "2024-03-28:10:04:05,101 INFO     [task.py:395] Building contexts for gsm8k on rank 0...\n",
            "100% 1319/1319 [00:08<00:00, 164.49it/s]\n",
            "2024-03-28:10:04:13,150 INFO     [evaluator.py:379] Running generate_until requests\n",
            "Running generate_until requests:   0% 0/1319 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1027 > 1024). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1339: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "Running generate_until requests: 100% 1319/1319 [37:35<00:00,  1.71s/it]\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "huggingface (pretrained=openai-community/gpt2,num_beams=3,temperature=1), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
            "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
            "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
            "|gsm8k|      3|strict-match    |     5|exact_match|0.0038|±  |0.0017|\n",
            "|     |       |flexible-extract|     5|exact_match|0.0205|±  |0.0039|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!lm_eval \\\n",
        "  --model huggingface \\\n",
        "  --model_args pretrained=openai-community/gpt2,num_beams=3,temperature=1 \\\n",
        "  --tasks gsm8k \\\n",
        "  --batch_size 2 \\\n",
        "  --device cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB5GAadkqF9y"
      },
      "source": [
        "Running generate_until requests: 100% 1319/1319 [37:35<00:00,  1.71s/it]\n",
        "\n",
        "huggingface (pretrained=openai-community/gpt2,num_beams=3,temperature=1), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
        "\n",
        "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
        "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
        "|gsm8k|      3|strict-match    |     5|exact_match|0.0038|±  |0.0017|\n",
        "|     |       |flexible-extract|     5|exact_match|0.0205|±  |0.0039|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyaQLi8zV5ap"
      },
      "source": [
        "temp, beam = (1, 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTDGNHlDVtqC",
        "outputId": "23d6f1c6-244c-43c6-8b9c-4f5b82bf6cb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-28 10:43:28.081228: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-28 10:43:28.081284: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-28 10:43:28.082682: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-28 10:43:29.260757: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-03-28:10:43:32,831 INFO     [__main__.py:251] Verbosity set to INFO\n",
            "2024-03-28:10:43:38,338 INFO     [__main__.py:335] Selected Tasks: ['gsm8k']\n",
            "2024-03-28:10:43:38,340 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-03-28:10:43:38,340 INFO     [evaluator.py:177] Initializing huggingface model, with arguments: {'pretrained': 'openai-community/gpt2', 'num_beams': 6, 'temperature': 1}\n",
            "2024-03-28:10:43:38,382 INFO     [huggingface.py:163] Using device 'cuda'\n",
            "2024-03-28:10:43:41,641 INFO     [task.py:395] Building contexts for gsm8k on rank 0...\n",
            "100% 1319/1319 [00:08<00:00, 159.85it/s]\n",
            "2024-03-28:10:43:49,920 INFO     [evaluator.py:379] Running generate_until requests\n",
            "Running generate_until requests:   0% 0/1319 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1027 > 1024). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1339: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "Running generate_until requests: 100% 1319/1319 [1:04:53<00:00,  2.95s/it]\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "huggingface (pretrained=openai-community/gpt2,num_beams=6,temperature=1), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
            "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
            "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
            "|gsm8k|      3|strict-match    |     5|exact_match|0.0030|±  |0.0015|\n",
            "|     |       |flexible-extract|     5|exact_match|0.0159|±  |0.0034|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!lm_eval \\\n",
        "  --model huggingface \\\n",
        "  --model_args pretrained=openai-community/gpt2,num_beams=6,temperature=1 \\\n",
        "  --tasks gsm8k \\\n",
        "  --batch_size 2 \\\n",
        "  --device cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqko1xZv__oz"
      },
      "source": [
        "Running generate_until requests: 100% 1319/1319 [1:04:53<00:00,  2.95s/it]\n",
        "\n",
        "huggingface (pretrained=openai-community/gpt2,num_beams=6,temperature=1), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
        "\n",
        "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
        "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
        "|gsm8k|      3|strict-match    |     5|exact_match|0.0030|±  |0.0015|\n",
        "|     |       |flexible-extract|     5|exact_match|0.0159|±  |0.0034|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZEzowLW0tDV"
      },
      "source": [
        "## Running beam size = 3, temperature = 0.7 with Different Seed Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzC5ZXSprKNx"
      },
      "source": [
        "To make sure that the optimal value is not due to random sampling, we repeatedly executed the same model with beam size 3 and temperature 0.7, changing the seed values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJY1g6zGSH2y",
        "outputId": "9b9583fa-4d5b-40a1-8584-77b9734e304e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-04-02 15:39:39.175807: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-02 15:39:39.175862: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-02 15:39:39.177334: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-02 15:39:40.492795: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading builder script: 100% 5.67k/5.67k [00:00<00:00, 18.0MB/s]\n",
            "2024-04-02:15:39:46,312 INFO     [__main__.py:251] Verbosity set to INFO\n",
            "2024-04-02:15:39:52,600 INFO     [__main__.py:335] Selected Tasks: ['gsm8k']\n",
            "2024-04-02:15:39:52,607 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-04-02:15:39:52,607 INFO     [evaluator.py:177] Initializing huggingface model, with arguments: {'pretrained': 'openai-community/gpt2', 'num_beams': 3, 'temperature': 0.7}\n",
            "2024-04-02:15:39:52,681 INFO     [huggingface.py:163] Using device 'cuda'\n",
            "config.json: 100% 665/665 [00:00<00:00, 3.69MB/s]\n",
            "model.safetensors: 100% 548M/548M [00:04<00:00, 135MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "generation_config.json: 100% 124/124 [00:00<00:00, 588kB/s]\n",
            "tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 132kB/s]\n",
            "vocab.json: 100% 1.04M/1.04M [00:00<00:00, 9.01MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 9.54MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 23.5MB/s]\n",
            "Downloading readme: 100% 7.94k/7.94k [00:00<00:00, 23.4MB/s]\n",
            "Downloading data: 100% 2.31M/2.31M [00:00<00:00, 4.19MB/s]\n",
            "Downloading data: 100% 419k/419k [00:00<00:00, 946kB/s]\n",
            "Generating train split: 100% 7473/7473 [00:00<00:00, 152932.04 examples/s]\n",
            "Generating test split: 100% 1319/1319 [00:00<00:00, 322507.11 examples/s]\n",
            "2024-04-02:15:40:03,900 INFO     [task.py:395] Building contexts for gsm8k on rank 0...\n",
            "100% 1319/1319 [00:09<00:00, 140.28it/s]\n",
            "2024-04-02:15:40:13,335 INFO     [evaluator.py:379] Running generate_until requests\n",
            "Running generate_until requests:   0% 0/1319 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1027 > 1024). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1339: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "Running generate_until requests: 100% 1319/1319 [39:07<00:00,  1.78s/it]\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "huggingface (pretrained=openai-community/gpt2,num_beams=3,temperature=0.7), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
            "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
            "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
            "|gsm8k|      3|strict-match    |     5|exact_match|0.0038|±  |0.0017|\n",
            "|     |       |flexible-extract|     5|exact_match|0.0205|±  |0.0039|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(21)\n",
        "!lm_eval \\\n",
        "  --model huggingface \\\n",
        "  --model_args pretrained=openai-community/gpt2,num_beams=3,temperature=0.7 \\\n",
        "  --tasks gsm8k \\\n",
        "  --batch_size 2 \\\n",
        "  --device cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "044w2wBd-Ur1"
      },
      "source": [
        "Running generate_until requests: 100% 1319/1319 [39:07<00:00,  1.78s/it]\n",
        "\n",
        "huggingface (pretrained=openai-community/gpt2,num_beams=3,temperature=0.7), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
        "\n",
        "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
        "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
        "|gsm8k|      3|strict-match    |     5|exact_match|0.0038|±  |0.0017|\n",
        "|     |       |flexible-extract|     5|exact_match|0.0205|±  |0.0039|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9VVt2ZR-JmT",
        "outputId": "db0aef49-d79d-4593-cf1c-c12a12373652"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-04-02 18:01:24.123589: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-02 18:01:24.123640: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-02 18:01:24.125058: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-02 18:01:25.381191: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading builder script: 100% 5.67k/5.67k [00:00<00:00, 20.2MB/s]\n",
            "2024-04-02:18:01:32,286 INFO     [__main__.py:251] Verbosity set to INFO\n",
            "2024-04-02:18:01:38,244 INFO     [__main__.py:335] Selected Tasks: ['gsm8k']\n",
            "2024-04-02:18:01:38,250 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-04-02:18:01:38,250 INFO     [evaluator.py:177] Initializing huggingface model, with arguments: {'pretrained': 'openai-community/gpt2', 'num_beams': 3, 'temperature': 0.7}\n",
            "2024-04-02:18:01:38,323 INFO     [huggingface.py:163] Using device 'cuda'\n",
            "config.json: 100% 665/665 [00:00<00:00, 3.55MB/s]\n",
            "model.safetensors: 100% 548M/548M [00:01<00:00, 369MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "generation_config.json: 100% 124/124 [00:00<00:00, 625kB/s]\n",
            "tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 139kB/s]\n",
            "vocab.json: 100% 1.04M/1.04M [00:00<00:00, 1.31MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 765kB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 1.69MB/s]\n",
            "Downloading readme: 100% 7.94k/7.94k [00:00<00:00, 24.7MB/s]\n",
            "Downloading data: 100% 2.31M/2.31M [00:00<00:00, 5.18MB/s]\n",
            "Downloading data: 100% 419k/419k [00:00<00:00, 1.81MB/s]\n",
            "Generating train split: 100% 7473/7473 [00:00<00:00, 162517.16 examples/s]\n",
            "Generating test split: 100% 1319/1319 [00:00<00:00, 354361.19 examples/s]\n",
            "2024-04-02:18:01:54,734 INFO     [task.py:395] Building contexts for gsm8k on rank 0...\n",
            "100% 1319/1319 [00:08<00:00, 152.41it/s]\n",
            "2024-04-02:18:02:03,417 INFO     [evaluator.py:379] Running generate_until requests\n",
            "Running generate_until requests:   0% 0/1319 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1027 > 1024). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1339: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "Running generate_until requests: 100% 1319/1319 [39:42<00:00,  1.81s/it]\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "huggingface (pretrained=openai-community/gpt2,num_beams=3,temperature=0.7), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
            "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
            "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
            "|gsm8k|      3|strict-match    |     5|exact_match|0.0038|±  |0.0017|\n",
            "|     |       |flexible-extract|     5|exact_match|0.0205|±  |0.0039|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "!lm_eval \\\n",
        "  --model huggingface \\\n",
        "  --model_args pretrained=openai-community/gpt2,num_beams=3,temperature=0.7 \\\n",
        "  --tasks gsm8k \\\n",
        "  --batch_size 2 \\\n",
        "  --device cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxkquMkIMZ9O"
      },
      "source": [
        "Running generate_until requests: 100% 1319/1319 [39:42<00:00,  1.81s/it]\n",
        "\n",
        "huggingface (pretrained=openai-community/gpt2,num_beams=3,temperature=0.7), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
        "\n",
        "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
        "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
        "|gsm8k|      3|strict-match    |     5|exact_match|0.0038|±  |0.0017|\n",
        "|     |       |flexible-extract|     5|exact_match|0.0205|±  |0.0039|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GAk8jzrMTlW",
        "outputId": "45b1cde3-20c8-485f-c1c8-900124c33f64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-04-02 19:03:06.296517: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-02 19:03:06.296578: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-02 19:03:06.298217: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-02 19:03:07.583657: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-04-02:19:03:12,185 INFO     [__main__.py:251] Verbosity set to INFO\n",
            "2024-04-02:19:03:18,362 INFO     [__main__.py:335] Selected Tasks: ['gsm8k']\n",
            "2024-04-02:19:03:18,363 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-04-02:19:03:18,364 INFO     [evaluator.py:177] Initializing huggingface model, with arguments: {'pretrained': 'openai-community/gpt2', 'num_beams': 3, 'temperature': 0.7}\n",
            "2024-04-02:19:03:18,408 INFO     [huggingface.py:163] Using device 'cuda'\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "2024-04-02:19:03:23,583 INFO     [task.py:395] Building contexts for gsm8k on rank 0...\n",
            "100% 1319/1319 [00:08<00:00, 154.74it/s]\n",
            "2024-04-02:19:03:32,135 INFO     [evaluator.py:379] Running generate_until requests\n",
            "Running generate_until requests:   0% 0/1319 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1027 > 1024). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1339: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "Running generate_until requests: 100% 1319/1319 [39:19<00:00,  1.79s/it]\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "huggingface (pretrained=openai-community/gpt2,num_beams=3,temperature=0.7), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
            "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
            "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
            "|gsm8k|      3|strict-match    |     5|exact_match|0.0038|±  |0.0017|\n",
            "|     |       |flexible-extract|     5|exact_match|0.0205|±  |0.0039|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(54)\n",
        "!lm_eval \\\n",
        "  --model huggingface \\\n",
        "  --model_args pretrained=openai-community/gpt2,num_beams=3,temperature=0.7 \\\n",
        "  --tasks gsm8k \\\n",
        "  --batch_size 2 \\\n",
        "  --device cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uYUVttRf0Pg"
      },
      "source": [
        "Running generate_until requests: 100% 1319/1319 [39:19<00:00,  1.79s/it]\n",
        "\n",
        "huggingface (pretrained=openai-community/gpt2,num_beams=3,temperature=0.7), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
        "\n",
        "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
        "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
        "|gsm8k|      3|strict-match    |     5|exact_match|0.0038|±  |0.0017|\n",
        "|     |       |flexible-extract|     5|exact_match|0.0205|±  |0.0039|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuEKTi-Ql7iZ"
      },
      "source": [
        "Running generate_until requests: 100% 1319/1319 [42:23<00:00,  1.93s/it]\n",
        "\n",
        "huggingface (pretrained=openai-community/gpt2,num_beams=3,temperature=0.7), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2\n",
        "\n",
        "|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|\n",
        "|-----|------:|----------------|-----:|-----------|-----:|---|-----:|\n",
        "|gsm8k|      3|strict-match    |     5|exact_match|0.0038|±  |0.0017|\n",
        "|     |       |flexible-extract|     5|exact_match|0.0205|±  |0.0039|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DuoPMLIrYSZ"
      },
      "source": [
        "The output is the same at 0.0205 for every seed set, which confirms that our findings were not the result of random sampling."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "010cc5525bfb4fb3ae3c13528f4ecd8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d40243d3f54349348549d94c04cbab4d",
            "max": 1319,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_171fed95e9ce4f2096819c2d1495f1f0",
            "value": 1319
          }
        },
        "04aec520810d4f54af7bfbe3ba966d70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "171fed95e9ce4f2096819c2d1495f1f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "17a4a694258640fa9d3273c0aa3d5d1d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27484c9a98cc484abb5a546dc4ce0da2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57cdd688de41484d8864974901aa1c8d",
            "placeholder": "​",
            "style": "IPY_MODEL_805e7171ba974f1b8e5a08baed665987",
            "value": "Generating test split: 100%"
          }
        },
        "32081d48ffb64299a62ea1a13df64699": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_27484c9a98cc484abb5a546dc4ce0da2",
              "IPY_MODEL_010cc5525bfb4fb3ae3c13528f4ecd8e",
              "IPY_MODEL_866b32a4c9804827aab1b11246bedafd"
            ],
            "layout": "IPY_MODEL_3a7ceecd85d14c2b80b56a70ea37184f"
          }
        },
        "34d4b86e617340d1aba190969adf3290": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a7ceecd85d14c2b80b56a70ea37184f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bbb9cfbd3344c6daa4c12de617c5e2c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c10db06c78549a98a96b1a072c25534": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8e963b308914a06bd24f17ff764d862",
            "max": 7940,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_82f211ed93d746df963b8f82cdac304c",
            "value": 7940
          }
        },
        "480c9384c571426e8b73162dcdfc116e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4cb73d299cb44c21895fe0bf3977e149": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4dde6c751b804cfe8278b1fcca234790": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f2174d59f444ea2bdaa48ef3ccf2442": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "515fffdf9f7542f39b013fac3a61aaaf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "516fc5911376462da569f469d3be8678": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_811fd4df676e46caa4464f8e1c521105",
            "max": 419088,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5c25208686ff4643939966a6ecc71595",
            "value": 419088
          }
        },
        "531c3078132c4f978011431c572dc0ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17a4a694258640fa9d3273c0aa3d5d1d",
            "placeholder": "​",
            "style": "IPY_MODEL_54ce31c9fd484faaad3b4a391ee1f114",
            "value": " 419k/419k [00:00&lt;00:00, 4.20MB/s]"
          }
        },
        "5348bf2b776e4cb590c875eb39b1a90e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b854d43ad4cb4b7893f3042fd3805bf0",
              "IPY_MODEL_fbb54ec6beb142aaa03b3834fa13f212",
              "IPY_MODEL_fd9da52635d4481b8e67f003176a7c1b"
            ],
            "layout": "IPY_MODEL_3bbb9cfbd3344c6daa4c12de617c5e2c"
          }
        },
        "54ce31c9fd484faaad3b4a391ee1f114": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57cdd688de41484d8864974901aa1c8d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58931497709446e389555f44e336b340": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca77742883a147ad8aa58dbb656a6dd8",
            "placeholder": "​",
            "style": "IPY_MODEL_04aec520810d4f54af7bfbe3ba966d70",
            "value": "Generating train split: 100%"
          }
        },
        "5c25208686ff4643939966a6ecc71595": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d2a8185fd5347488e065351852b60c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_994b6a26623a4443b7ee62cf785650df",
            "placeholder": "​",
            "style": "IPY_MODEL_a6aab5263b3441fe9bf3c9b6793a202d",
            "value": "Downloading data: 100%"
          }
        },
        "5e683231ddf5486ebbe33c6a9c265527": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64aa5169e9ca44b2b5e775307201ee9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b182d22fcc149eeaf1d6fa92bad8f9a",
            "placeholder": "​",
            "style": "IPY_MODEL_7acf39f3b7464a839725591532c4cb37",
            "value": " 7473/7473 [00:00&lt;00:00, 132655.19 examples/s]"
          }
        },
        "6c69e330ba7d459f8024240ad9ffec99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9aedeb21ea994f7e878d6873298e29b0",
              "IPY_MODEL_3c10db06c78549a98a96b1a072c25534",
              "IPY_MODEL_d0c17f39fe904c1aa97332afab726573"
            ],
            "layout": "IPY_MODEL_4dde6c751b804cfe8278b1fcca234790"
          }
        },
        "705a1b6458a74fe795968559357c9b41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "715b81f8e6ac4ae2aadab9a3b4ea6fb8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "728ae5368fbb408da40aee58f08f1fa5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7acf39f3b7464a839725591532c4cb37": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "805e7171ba974f1b8e5a08baed665987": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "811fd4df676e46caa4464f8e1c521105": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82f211ed93d746df963b8f82cdac304c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "866b32a4c9804827aab1b11246bedafd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb92de362fb0468593fe39b3bd101b03",
            "placeholder": "​",
            "style": "IPY_MODEL_e493fc3a98644a09803754829695044a",
            "value": " 1319/1319 [00:00&lt;00:00, 77509.06 examples/s]"
          }
        },
        "867eae22737a4843a82b5d242cbbf511": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b182d22fcc149eeaf1d6fa92bad8f9a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "994b6a26623a4443b7ee62cf785650df": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9aedeb21ea994f7e878d6873298e29b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0e3c68d5f794f119aef94ebaf3ccba5",
            "placeholder": "​",
            "style": "IPY_MODEL_4cb73d299cb44c21895fe0bf3977e149",
            "value": "Downloading readme: 100%"
          }
        },
        "9f9864b6320b45a480541ac17a3f0cf1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a27000a9aecd4aef98685a72152ac105": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_715b81f8e6ac4ae2aadab9a3b4ea6fb8",
            "max": 7473,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4e2f532dc6d4b36b53afe3806994934",
            "value": 7473
          }
        },
        "a6aab5263b3441fe9bf3c9b6793a202d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0e3c68d5f794f119aef94ebaf3ccba5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b854d43ad4cb4b7893f3042fd3805bf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_867eae22737a4843a82b5d242cbbf511",
            "placeholder": "​",
            "style": "IPY_MODEL_480c9384c571426e8b73162dcdfc116e",
            "value": "Downloading data: 100%"
          }
        },
        "b8e963b308914a06bd24f17ff764d862": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca77742883a147ad8aa58dbb656a6dd8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb92de362fb0468593fe39b3bd101b03": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0c17f39fe904c1aa97332afab726573": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_728ae5368fbb408da40aee58f08f1fa5",
            "placeholder": "​",
            "style": "IPY_MODEL_4f2174d59f444ea2bdaa48ef3ccf2442",
            "value": " 7.94k/7.94k [00:00&lt;00:00, 648kB/s]"
          }
        },
        "d30da0ce312c4ad59cdf46716303f969": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5d2a8185fd5347488e065351852b60c1",
              "IPY_MODEL_516fc5911376462da569f469d3be8678",
              "IPY_MODEL_531c3078132c4f978011431c572dc0ca"
            ],
            "layout": "IPY_MODEL_5e683231ddf5486ebbe33c6a9c265527"
          }
        },
        "d3b1feb92ab14c4cb79000bf0039fc69": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_58931497709446e389555f44e336b340",
              "IPY_MODEL_a27000a9aecd4aef98685a72152ac105",
              "IPY_MODEL_64aa5169e9ca44b2b5e775307201ee9d"
            ],
            "layout": "IPY_MODEL_515fffdf9f7542f39b013fac3a61aaaf"
          }
        },
        "d40243d3f54349348549d94c04cbab4d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da671f7378564f2e8f70b8933bdacbdc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e493fc3a98644a09803754829695044a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4e2f532dc6d4b36b53afe3806994934": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fbb54ec6beb142aaa03b3834fa13f212": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f9864b6320b45a480541ac17a3f0cf1",
            "max": 2306545,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_705a1b6458a74fe795968559357c9b41",
            "value": 2306545
          }
        },
        "fd9da52635d4481b8e67f003176a7c1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da671f7378564f2e8f70b8933bdacbdc",
            "placeholder": "​",
            "style": "IPY_MODEL_34d4b86e617340d1aba190969adf3290",
            "value": " 2.31M/2.31M [00:00&lt;00:00, 2.93MB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
